%% This is an example first chapter.  You should put chapter/appendix that you
%% write into a separate file, and add a line \include{yourfilename} to
%% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.
%% You can process specific files by typing their names in at the 
%% \files=
%% prompt when you run the file main.tex through LaTeX.




\chapter{Introduction}


Since the introduction of the milestone work of AlexNet \cite{alexnet??} the architecture of deep convolutional neutral networks (CNNs) has been constantly evolving over the years, becoming more accurate and faster. 

Many studies have focused on designign new architectures to increase accuracy for image classification on Imagnet without considering speed of the models, but real-world systems often require high recognition accuracy with low latency.

There are papers which focus on accuracy or low parameter count or improving the models throught modified training recipes but no one (really no one???) have combined all this constarains together

But 


не могу писать на английском, потому что постоянно отвлекаюсь и пытаюсь копировать, а нужно изливать свои мысли, так будет быстрее (((




% copy-paste
% The architecture of deep convolutional neutral networks (CNNs) has evolved for years, becoming more accurate and faster. Since the milestone work of AlexNet [1],
% the ImageNet classification accuracy has been significantly improved by novel structures, including VGG [2], GoogLeNet [3], ResNet [4,5], DenseNet [6], ResNeXt [7], SE-Net [8], and automatic neutral architecture search [9,10,11], to name a few.


Something goes here

intro is close to \cite{lin2020neural_genet} & \cite{ridnik2021_tresnet}

О чём блять вообще диплом? хочу быструю и одновременно качественную сетку. 
Многие работы принимают во внимание только качество, но для практических применений очень важна именно скорость, поэтому мы хотим постоянно иметь её во внимание. Есть много разных акслелераторов (см. отдельную главу о них), но эта работа фокусируется исключительно на современных видеокартах. Полученные выводы могут не выполняться для мобильных платформ (тут ссылочки на работы об этом). 

\chapter{Chapter2 (Speed)}


% bottlenecks induced by FLOPs oriented optimizations - говорю о том, что определине "маленькая" сеть и "быстрая" сеть неоднозначны. можно смотреть на параметры, можно на FLOPS, можно на MAC, можно на real-life performance. ссылаюсь на работы которые показывают что связь между FLOPS | params | mac и скоростью нифига не линейная. можно вставить графики сравения по FLOPS похожие на те что в \cite{ma2018shufflenetv2}

\section{Intro (again)}

Real world tasks often have computational constarains on the design of arcitecures. Design of light-weight models with good speed-accuracy tradeoffs has been explored extensively before \cite{Xception} \cite{mobilenetv1} \cite{mobilenetv2} \cite{ma2018shufflenetv2} \cite{zhang2018shufflenet}. 

To measure complexity of the model some works use number of computations it does. Examples of such metrics are floating point operations (FLOPs) and multiply-accumulate operations (MACCs). Other focus on minimizing of number of trainable parameters. This metrics are indirect approximation of direct metric such as speed or latency. Previous works has shown \cite{design_spaces} that factors affecting inference latency on modern accelerators are complicated and direct measurement on target device should be preffered. (тут можно еще цитирований из \cite{ma2018shufflenetv2} p.2 набрать).

There are many accelerators available, this works focuses on optimizing and developing networks specially optimized for moden GPUs. 


\sbusection{Definitions}
In this work, the definition of FLOPs follows \cite{zhang2018shufflenet}, i.e. number of multiply-adds

\subsection{Why not FLOPS}
в статьях HardNet и Searching for Fast Model Families on Datacenter Accelerators (и блог посте из презентации) есть детальное описание того, почему FLOPS не работате и 

(этот кусок скопирован и почти не переписан из Searching....) 
appications are either compute-bounded or memory-(bandwidth)-bound as they don't fit in on-chip memories. To achieve peak performance model's operational intensity is siffuicient ot push it into the saturation (i.e. compute-bound) region of the roofline.  

Тут снова нужно сказать о том, что современные модели memory bounded, not flops bounded. мы можем использовать это и иметь бОльше ФЛОПс и параметров, без уменьшения скорости. ФЛОПс повышают capacity сети, что хорошо. 


shows that the factors affecting the inference latency on modern GPUs are rather complicated



Definition of FLOPs .....

(not to be confused with Floating Point OPerations per Second - FLOPS which are used to measure the performance of hardware). 


Definition of MAC...
Defininiot of number of trainable parameters


\subsection{Examples of optimization}
Тут хочется привести пример Space2Depth модуля, который повышает эффективность за счет увеличения плотности операций в начале сети. 

% кусок нагло спизжен из Searching for Fast .... нужно будет переписать. 
еще про space2depth написано в статье \cite{ridnik2021_tresnet}


3.1. Efficient space-to-depth and space-to-batch
As pointed out in Section 2, convolutions need high parallelism in all dimensions (depth, batch, and spatial) to achieve high speed on TPUs and GPUs. However, insufficient parallelism because of the small depth and batch is the usual cause of low utilization and low performance on matrix units. We augment the search space with accelerator-friendly spaceto-depth and space-to-batch ops to increase depth and batch dimensions while keeping the total tensor volume the same. For space-to-depth ops, instead of using the memorycopy-reshape based ops provided by frameworks such as TensorFlow [6] and Pytorch [42], we customize an $n \times n$ convolution with stride-n to perform the space-to-depth operation, reshaping an $H \times W \times C$ tensor to an $H / n \times W / n \times C * n^{2}$ tensor. This approach has two advantages: 1) convolutions are much preferred by TPUs and GPUs because of their high operational intensity and execution efficiency; 2) in addition to reshaping the input tensor to improve operational intensity and efficiency, the $n \times n$ convolutions can also be trained

Chapter2.1


Chapter3 (Accuracy)
Трудно сравнивать чужие модели и архитектуры, потому что 1. не совпадают рецепты обучения 2. некоторые вещи не указывают в статьях, они есть только в коде, поэтому важно учить на своей код базе, чтобы убедиться что все одинаковое. 

(тут ли?)
Говорю о том что в последнее время появилось много малненьких улучшений, которые не замедляют сеть, но дают буст по качетсву \cite{zhang2019making_aa_shift_invariant} или space2depth  \cite{ridnik2021_tresnet} в начале сети. были статья которые объединяли это все вместе \cite{lee2020compounding_improvements} \cite{bello2021revisiting_resnet} (ну и как бы мои результаты не лучше чем у них, просто они тупо стакают все изменения на резнет, а я еще и архитектуру меняю после этого, чтобы учесть архитектруные недостатки


Глава2 - про скорость и связь со флопсами и все такое
Глава3 - про улучшения обучения связанные с ванильным R50
Глава4 - про связку небольших изменений и медленные изменения архитектуры опираясь на то, что работает у других
Глава5 - design choice not present in current literature


\section{Motivations for micro-optimization}

Text inside first section

\subsection{Post Multiply Normalization}

Text inside subsection
