%% This is an example first chapter.  You should put chapter/appendix that you
%% write into a separate file, and add a line \include{yourfilename} to
%% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.
%% You can process specific files by typing their names in at the 
%% \files=
%% prompt when you run the file main.tex through LaTeX.


\chapter{Introduction}


% copy-paste
% The architecture of deep convolutional neutral networks (CNNs) has evolved for years, becoming more accurate and faster. Since the milestone work of AlexNet [1],
% the ImageNet classification accuracy has been significantly improved by novel structures, including VGG [2], GoogLeNet [3], ResNet [4,5], DenseNet [6], ResNeXt [7], SE-Net [8], and automatic neutral architecture search [9,10,11], to name a few.

Since the introduction of the milestone work of AlexNet \cite{alexnet??} the architecture of deep convolutional neutral networks (CNNs) has been constantly evolving over the years, becoming more accurate and faster. 

Many studies have focused on designign new architectures to increase accuracy for image classification on Imagnet without considering speed of the models, but real-world systems often require high recognition accuracy with low latency.

There are papers which focus on imroving speed by reducing number of computations inside network or by minimizing number of parameters used. In Chapter 2 we discuss why such optimization don't always result in low inference speed on real-life accelerators.


In Chapter 3 we discuss recent modifications for training recipes and minor changes to architectures which improve model performance without compromising speed

In Chapter 4 we propose a novel desing choice not discussed in the literature before which is one of main points of this paper

In Chapter 5 we show result of out experiments which try to balance on speed-accuracy tradeoff by applying only changes that have minor impact on speed. By combining multple minor changes we obtain close to SOTA performance on large-scale image recogntiong dataset ImageNet. 


не могу писать на английском, потому что постоянно отвлекаюсь и пытаюсь копировать, а нужно изливать свои мысли, так будет быстрее (((


Something goes here

intro is close to \cite{lin2020neural_genet} & \cite{ridnik2021_tresnet}

О чём блять вообще диплом? хочу быструю и одновременно качественную сетку. 
Многие работы принимают во внимание только качество, но для практических применений очень важна именно скорость, поэтому мы хотим постоянно иметь её во внимание. Есть много разных акслелераторов (см. отдельную главу о них), но эта работа фокусируется исключительно на современных видеокартах. Полученные выводы могут не выполняться для мобильных платформ (тут ссылочки на работы об этом). 