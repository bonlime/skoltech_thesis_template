
The last several years introduced a huge leap forward in the domain of designing novel convolutional architectures for computer vision. A variety of training techniques has been introduced, combined with modifications for models themselves. While the architecture design attracts a lot of attention it often conceals the improvement that comes from modified regularization and augmentation techniques. Our work revisits the classical ResNet (He et al., 2015) architecture and tries to disentangle the performance gains that come from the model modifications from the impact of training tricks. We also study the additive nature of such tricks by combining multiple existing techniques together for achieving a noticeable improvement. Perhaps surprisingly, our findings show that training strategies are as important as changes in architecture. Extensive experiments of assembling these strategies together improve top-1 accuracy of ResNet50 from 76.5\% to 79.6\% without any changes in the model. With subtle changes, which almost don't affect the inference speed, the performance could additionally be boosted to 81.2\% which is on par with current State of The Art models. The main conclusion from our work is that modified and revised ResNet-50 still could be used as a strong baseline for future research and for real-life applications.