%% This is an example first chapter.  You should put chapter/appendix that you
%% write into a separate file, and add a line \include{yourfilename} to
%% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.
%% You can process specific files by typing their names in at the 
%% \files=
%% prompt when you run the file main.tex through LaTeX.


\chapter{Introduction}


% copy-paste
% The architecture of deep convolutional neural networks (CNNs) has evolved for years, becoming more accurate and faster. Since the milestone work of AlexNet [1],
% the ImageNet classification accuracy has been significantly improved by novel structures, including VGG [2], GoogLeNet [3], ResNet [4,5], DenseNet [6], ResNeXt [7], SE-Net [8], and automatic neutral architecture search [9,10,11], to name a few.


% intro is close to \cite{lin2020neural_genet} & \cite{ridnik2021_tresnet}

% Многие работы принимают во внимание только качество, но для практических применений очень важна именно скорость, поэтому мы хотим постоянно иметь её во внимание. Есть много разных акслелераторов (см. отдельную главу о них), но эта работа фокусируется исключительно на современных видеокартах. Полученные выводы могут не выполняться для мобильных платформ (тут ссылочки на работы об этом). 


Introduction of  AlexNet \cite{krizhevsky2012_imagenet_alexnet} set a new milestone for future evolution of convolutional neural networks (CNNs) architectures. Since then CNNs have become faster and more accurate. Many studies have focused on designing new architectures to increase accuracy for image classification on ImageNet without considering the speed of the models, but real-world systems often require high recognition accuracy with low latency. Some papers focus on improving speed by reducing the number of computations inside a network or by minimizing the number of parameters used. In Chapter 2 \ref{chap:speed} we discuss why such optimization doesn't always result in low inference speed on real-life accelerators. And what limits the speed of deep CNNs and how to mitigate speed bottlenecks.

In the following Chapter3 \ref{chap:performance} we define what "performance" means, define the optimization objective in more detail, and discuss recent modifications for training recipes and minor changes to architectures that improve model performance without compromising speed. In Chapter 4 we propose a novel design choice not discussed in the literature before which is one of the main points of this paper. In Chapter 5 we show the result of our experiments which try to balance on speed-accuracy trade-off by applying only changes that have a minor impact on speed. By combining multiple minor changes we obtain close to SOTA performance on large-scale image recognition dataset ImageNet. After that, we propose a model which includes all of the mentioned improvements into one and could be used as a powerful backbone for downstream tasks in other computer vision domains.

The main focus of this work is to design a special network that has high precision and can be inferenced very fast on modern GPUs.We show that taking into account current hardware constraints, rethinking block selection, and compounding minor improvements could stack up to an increase both in terms of accuracy and in terms of latency over the baseline. It's tightly related to the field of convolutional neural network architecture design, a field that is focused on improving the performance of the networks. The process of designing could be either manual or automatic. The manual design includes a human-in-the-loop to propose and evaluate the effect of newly introduced blocks and design principles. Architectures achieved by manual search usually have clear and simple structures and are easy to interpret. 

% this part was rewritten from Designing ...
Recently, the design process of new networks has shifted from a manual exploration to a Neural Architecture Search (NAS) \cite{real2019_nas1}. The majority of research related to NAS focuses on designing either an efficient search algorithm, i.e. how to find the best network instances efficiently \cite{zoph2018_nasnet} \cite{pham2018_nas_efficient} or designing a so-called "design space" which contains all possible model instances. \cite{radosavovic2020_designing} \cite{liu2018_nas_progressive}. NAS has already proven to be an efficient way of obtaining good models, \cite{tan2019_efficientnet} \cite{lin2020neural_genet} \cite{pham2018_nas_efficient}, however, while being a promising technique, NAS is computationally expensive and requires a large number of GPU hours for achieving good results. It also often leads to irregular structure of the network and can overfit to the given accelerator/dataset. Due to these problems, we do not explore automatic architecture search and focus on applying analysis and human judgment for developing architectures. While the proposed architecture could be improved further by applying NAS within the found design space, it lies outside of the scope of this work


% У Эльвиры оглавление такое (и это в целом похоже на нормальное оглавление диплома):
% Introduction
% CT overview
% Literature review
% Problem statement
% Methods for denoising 
% Data description
% Experiment and results
% Discussions
% Conclusions


