%% This is an example first chapter.  You should put chapter/appendix that you
%% write into a separate file, and add a line \include{yourfilename} to
%% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.
%% You can process specific files by typing their names in at the 
%% \files=
%% prompt when you run the file main.tex through LaTeX.


\chapter{Introduction}


% copy-paste
% The architecture of deep convolutional neutral networks (CNNs) has evolved for years, becoming more accurate and faster. Since the milestone work of AlexNet [1],
% the ImageNet classification accuracy has been significantly improved by novel structures, including VGG [2], GoogLeNet [3], ResNet [4,5], DenseNet [6], ResNeXt [7], SE-Net [8], and automatic neutral architecture search [9,10,11], to name a few.


% intro is close to \cite{lin2020neural_genet} & \cite{ridnik2021_tresnet}

% Многие работы принимают во внимание только качество, но для практических применений очень важна именно скорость, поэтому мы хотим постоянно иметь её во внимание. Есть много разных акслелераторов (см. отдельную главу о них), но эта работа фокусируется исключительно на современных видеокартах. Полученные выводы могут не выполняться для мобильных платформ (тут ссылочки на работы об этом). 


Since the introduction of the milestone work of AlexNet \cite{krizhevsky2012_imagenet_alexnet} the architecture of deep convolutional neutral networks (CNNs) has been constantly evolving over the years, becoming faster and more accurate. Many studies have focused on designign new architectures to increase accuracy for image classification on Imagnet without considering speed of the models, but real-world systems often require high recognition accuracy with low latency. There are papers which focus on improving speed by reducing number of computations inside network or by minimizing number of parameters used. In Chapter 2 \ref{chap:speed} we discuss why such optimization don't always result in low inference speed on real-life accelerators. And what limits the speed of deep CNNs and how to mutigate speed bottlenecks.

In the following Chapter3 \ref{chap:performance} we define what "performance" means, define the optimization objective in more details and discuss recent modifications for training recipes and minor changes to architectures which improve model performance without compromising speed. In Chapter 4 we propose a novel design choice not discussed in the literature before which is one of main points of this paper. In Chapter 5 we show result of out experiments which try to balance on speed-accuracy trade off by applying only changes that have minor impact on speed. By combining multiple minor changes we obtain close to SOTA performance on large-scale image recognition dataset ImageNet. After that we propose a model which includes all of the mentioned improvements into one and could be used as a power backbone for down-stream tasks in other computer vision domains.

This paper focuses on designing high precision network specifically optimized for fast inference speed on modern GPUs. We show that taking into account current hardware constrains, rethinking block selection and compounding minor improvements could stack up to an increase both in terms of accuracy and in term of latency over the baseline. It's tightly related to the filed of convolutional neural networks architecture design, a field which is focused on improving the performance the networks. The process of designing could be either manual or automatic. Manual design includes a human-in-the-loop to propose and evaluate effect of newly introduced blocks and design principles. Architectures achieved by manual search usually have clear and simple structure and are easy to interpret. 

% this part was rewritten from Designing ...
Recently, the design process of new networks has shifted from a manual exploration to Neural architecture search (NAS) \cite{real2019_nas1}. The majority of research related to NAS focuses on designing either an efficient search algorithm, i.e. how to find the best networks instances efficiently \cite{zoph2018_nasnet} \cite{pham2018_nas_efficient} or designing a so-called "design space" which contains all possible model instances. \cite{radosavovic2020_designing} \cite{liu2018_nas_progressive}. NAS has already proven to be an efficient way of obtaining good models, \cite{tan2019_efficientnet} \cite{lin2020neural_genet} \cite{pham2018_nas_efficient}, however while being a promising techniques, NAS is computationally expensive and requires large number of GPU hours for achieving good results. It also often leads to irregular structure of the network and can overfit to the given accelerator/dataset. Due to this problems we do not explore automatic architecture search and focus on applying analysis and human judgement for developing architectures. While the proposed architecture could be improved further by applying NAS withing the found desing space, it lies outside of scope of this work


% У Эльвиры оглавление такое (и это в целом похоже на нормальное оглавление диплома):
% Introduction
% CT overview
% Literature review
% Problem statement
% Methods for denoising 
% Data description
% Experiment and results
% Discussions
% Conclusions


